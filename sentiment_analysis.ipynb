{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "5264b493-8338-4a58-81f7-0c80f925626b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['tweet', 'sentiment', 'url'], dtype='object')\n",
      "tweets in dataset: 38089\n",
      "38091\n",
      "                                               tweet  sentiment\n",
      "0  BYND JPMorgan reels in expectations on Beyond ...          2\n",
      "1  CCL RCL Nomura points to bookings weakness at ...          2\n",
      "2  CX Cemex cut at Credit Suisse JP Morgan on wea...          2\n",
      "3                  ESS BTIG Research cuts to Neutral          2\n",
      "4       FNKO Funko slides after Piper Jaffray PT cut          2\n",
      "unique words in dataset: 46650\n",
      "[[2358, 1058, 7042, 5, 744, 9, 1297, 2742], [1090, 4300, 4576, 302, 2, 9656, 1298, 12, 3332, 6, 1825, 3850], [12178, 17168, 318, 12, 405, 1599, 1944, 728, 9, 667, 850, 610], [6328, 4577, 632, 510, 2, 915], [8063, 5763, 2622, 47, 2525, 4899, 621, 318], [7043, 12179, 1136, 12, 9657, 35, 593, 146, 906, 12, 1222, 208], [147, 147, 1600, 3, 444], [147, 1222, 208, 510, 2, 189], [8064, 4301, 510, 2, 45, 2526], [17169, 17170, 51, 12180, 510, 916, 1670, 47, 4900, 2359], [4302, 5292, 1441, 9, 17171], [17172, 4051, 12181, 3657, 1202, 5764, 621, 9, 17173], [11, 4052, 318, 2, 45, 2526, 12, 3851, 1671], [9658, 4578, 412, 510, 2, 130], [3333, 7044, 7045, 375, 64, 2623, 1497], [12182, 17174, 1136, 406, 4, 2843, 4579], [3852, 2959, 510, 2, 2526], [8065, 7046, 8065, 318, 12, 405, 1599, 9, 311, 3334, 27, 5765, 1907, 1106], [12183, 4053, 2159, 6329, 1908, 373, 17175, 8066], [12184, 2525, 578, 1, 17176, 4303], [17177, 17178, 241, 510, 2, 45, 2526], [195, 33, 603, 715, 1672, 36], [1183, 8067, 1600, 3, 444], [523, 4576, 5293, 1600, 1417, 5, 2276, 523, 869], [3188, 6330, 484, 17179, 104, 9, 677, 429], [17180, 17181, 17182, 17183, 17184, 498, 28, 8068, 406, 5, 29], [1673, 17185, 3067, 6331, 1468, 7047, 1345, 985, 2, 3189], [1673, 17186, 632, 510, 2, 1387, 2526], [5766, 4580, 510, 5766, 104, 9, 4304, 3190], [1136, 2, 611, 2844, 27, 1601, 12, 2960, 370], [2042, 6, 378, 37, 735, 318, 12, 1469, 9, 4305, 1203, 610], [544, 1908, 37, 104, 318, 2, 27, 12, 4053, 370], [683, 1223, 1136, 2, 915, 27, 30, 12, 720, 4306, 6332], [485, 1224, 3853, 17187, 1137, 610], [17188, 968, 1136, 2, 9659, 2526, 27, 1259, 12, 2959], [9660, 2527, 20, 37, 104, 318, 2, 27, 12, 720, 4306, 6332], [2160, 1136, 31, 6331, 2, 130, 9, 3335, 4307, 295, 4054, 1, 1042, 123], [2160, 79, 3658, 47, 6331, 2161], [2160, 20, 672, 47, 6331, 2159, 729, 153, 191, 575, 4901], [3191, 890, 20, 37, 104, 318, 2, 27, 12, 3068, 4581], [9661, 1136, 2, 4582, 27, 915, 12, 1058], [7048, 1441, 4, 171, 1346, 8, 12185], [1469, 9662, 9, 6333, 6334], [5767, 7049, 1136, 2, 1259, 27, 181, 30, 12, 3851, 1671], [5767, 7049, 1136, 2, 611, 2844, 27, 1601, 12, 2960, 370], [7050, 1998, 1136, 2, 45, 2526, 27, 1259, 12, 3192, 370], [7050, 1998, 1136, 2, 4582, 27, 611, 2844, 12, 2960, 370], [9663, 1642, 1136, 2, 5, 332, 27, 1259, 12, 4055, 5294], [9663, 1642, 20, 37, 104, 318, 2, 27, 12, 4055, 5294], [1138, 3193, 1136, 2, 30, 27, 181, 30, 12, 3194]]\n",
      "sequence length 38091\n",
      "[0. 0. 1.]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.12/site-packages/keras/src/layers/core/embedding.py:90: UserWarning: Argument `input_length` is deprecated. Just remove it.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential_10\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"sequential_10\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ embedding_10 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Embedding</span>)        │ ?                      │   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ global_max_pooling1d_7          │ ?                      │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">GlobalMaxPooling1D</span>)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_40 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                │ ?                      │   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_20 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)            │ ?                      │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_41 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                │ ?                      │   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_21 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)            │ ?                      │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_42 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                │ ?                      │   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_43 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                │ ?                      │   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ embedding_10 (\u001b[38;5;33mEmbedding\u001b[0m)        │ ?                      │   \u001b[38;5;34m0\u001b[0m (unbuilt) │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ global_max_pooling1d_7          │ ?                      │             \u001b[38;5;34m0\u001b[0m │\n",
       "│ (\u001b[38;5;33mGlobalMaxPooling1D\u001b[0m)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_40 (\u001b[38;5;33mDense\u001b[0m)                │ ?                      │   \u001b[38;5;34m0\u001b[0m (unbuilt) │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_20 (\u001b[38;5;33mDropout\u001b[0m)            │ ?                      │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_41 (\u001b[38;5;33mDense\u001b[0m)                │ ?                      │   \u001b[38;5;34m0\u001b[0m (unbuilt) │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_21 (\u001b[38;5;33mDropout\u001b[0m)            │ ?                      │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_42 (\u001b[38;5;33mDense\u001b[0m)                │ ?                      │   \u001b[38;5;34m0\u001b[0m (unbuilt) │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_43 (\u001b[38;5;33mDense\u001b[0m)                │ ?                      │   \u001b[38;5;34m0\u001b[0m (unbuilt) │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "\u001b[1m469/469\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 8ms/step - accuracy: 0.5456 - loss: 0.9653 - val_accuracy: 0.4313 - val_loss: 1.2367\n",
      "Epoch 2/5\n",
      "\u001b[1m469/469\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 7ms/step - accuracy: 0.7975 - loss: 0.5129 - val_accuracy: 0.4583 - val_loss: 1.2674\n",
      "Epoch 3/5\n",
      "\u001b[1m469/469\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 7ms/step - accuracy: 0.9065 - loss: 0.2637 - val_accuracy: 0.4439 - val_loss: 1.6775\n",
      "Epoch 4/5\n",
      "\u001b[1m469/469\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 7ms/step - accuracy: 0.9615 - loss: 0.1237 - val_accuracy: 0.4660 - val_loss: 1.9009\n",
      "Epoch 5/5\n",
      "\u001b[1m469/469\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 7ms/step - accuracy: 0.9841 - loss: 0.0579 - val_accuracy: 0.4537 - val_loss: 2.4336\n",
      "Max sequence length from training data: 688\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step\n",
      "[[1.12892635e-01 3.22376750e-03 8.83883655e-01]\n",
      " [1.62653741e-03 9.96737123e-01 1.63644704e-03]\n",
      " [4.17230129e-02 9.44970369e-01 1.33065525e-02]\n",
      " [4.38159078e-01 3.07605296e-01 2.54235625e-01]\n",
      " [3.17306811e-04 9.99501109e-01 1.81625655e-04]]\n",
      "Tweet: 'JPMorgan price will fall Its been a bad week for the company' => Predicted Sentiment: Negative\n",
      "Tweet: 'Stock price soars on great day production increases at the company' => Predicted Sentiment: Positive\n",
      "Tweet: 'I am going to be looking at the stock price later today' => Predicted Sentiment: Positive\n",
      "Tweet: 'There has been a change in management at chipotle' => Predicted Sentiment: Neutral\n",
      "Tweet: 'Profits soar for small startup with a big jump in sales' => Predicted Sentiment: Positive\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from keras.utils import to_categorical\n",
    "from keras import models\n",
    "from keras import layers\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "import re\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "df = pd.read_parquet(\"hf://datasets/TimKoornstra/financial-tweets-sentiment/data/train-00000-of-00001.parquet\")\n",
    "\n",
    "print(df.columns)\n",
    "df.drop(columns='url')\n",
    "\n",
    "print(f\"tweets in dataset: {len(np.unique(df[\"tweet\"]))}\")\n",
    "#print(df.head)\n",
    "\n",
    "pattern = r'[^\\w\\s]'\n",
    "\n",
    "# Use regex substitution to remove special characters from the 'text' column\n",
    "df['tweet'] = df['tweet'].apply(lambda x: re.sub(pattern, '', x))\n",
    "\n",
    "# Print the updated DataFrame\n",
    "#print([(np.unique(df[\"tweet\"]))])\n",
    "\n",
    "# Use ASCII filtering to remove non-ASCII characters from the 'text' column\n",
    "df['tweet'] = df['tweet'].apply(lambda x: ''.join(char for char in x if ord(char) < 128))\n",
    "#print('\\n\\n\\n')\n",
    "#print(([(np.unique(df[\"tweet\"]))]))\n",
    "print(len(df['tweet']))\n",
    "\n",
    "\n",
    "def remove_links(tweet):\n",
    "    words = tweet.split()  # Split tweet into words\n",
    "    filtered_words = [word for word in words if not word.startswith('http')]  # Remove words that start with 'http'\n",
    "    return ' '.join(filtered_words)  # Join words back together into a tweet\n",
    "\n",
    "# Apply the function to each entry in the 'tweets' column\n",
    "df['tweet'] = df['tweet'].apply(remove_links)\n",
    "df=df.drop(columns=['url'])\n",
    "#print(df.head)\n",
    "\n",
    "def remove_numerical_words(tweet):\n",
    "    # Use regex to remove words containing any digits\n",
    "    return ' '.join(word for word in tweet.split() if not re.search(r'\\d', word))\n",
    "\n",
    "# Apply the function to the 'tweet' column\n",
    "df['tweet'] = df['tweet'].apply(remove_numerical_words)\n",
    "\n",
    "# Check the updated DataFrame\n",
    "print(df.head())\n",
    "df.to_csv('out.csv', index=False) \n",
    "\n",
    "all_tweets = ' '.join(df['tweet'])\n",
    "\n",
    "# Step 2: Split the combined string into individual words\n",
    "all_words = all_tweets.split()\n",
    "\n",
    "# Step 3: Use a set to get unique words\n",
    "unique_words = set(all_words)\n",
    "\n",
    "# Convert the set back to a sorted list (optional)\n",
    "unique_words_list = sorted(list(unique_words))\n",
    "print(f'unique words in dataset: {len(unique_words_list)}')\n",
    "#print(unique_words_list)\n",
    "#\"Tokenizing\" data\n",
    "\n",
    "#df['sentiment'] = to_categorical(df['sentiment'].values, num_classes=3)\n",
    "\n",
    "# Tokenizing and padding sequences (same as before)\n",
    "tokenizer = Tokenizer(num_words=50000)\n",
    "tokenizer.fit_on_texts(df['tweet'])\n",
    "\n",
    "sequences = tokenizer.texts_to_sequences(df['tweet'])\n",
    "print(sequences[:50])\n",
    "print(f'sequence length {len(sequences)}')\n",
    "padded_sequences = pad_sequences(sequences, padding='post')\n",
    "max_sequence_length = padded_sequences.shape[1]\n",
    "\n",
    "# Split the data into training and test sets\n",
    "train_x = padded_sequences[:15000]\n",
    "train_y = df['sentiment'][:15000]\n",
    "test_x = padded_sequences[15000:]\n",
    "test_y = df['sentiment'][15000:]\n",
    "\n",
    "#turns this into one hot encoding format\n",
    "train_y = to_categorical(train_y, num_classes=3)\n",
    "test_y = to_categorical(test_y, num_classes=3)\n",
    "\n",
    "print(train_y[1])\n",
    "\n",
    "\n",
    "vocab_size=50000\n",
    "# Training the model\n",
    "model = models.Sequential()\n",
    "\n",
    "# Embedding layer (input length is max_sequence_length after padding)\n",
    "model.add(layers.Embedding(input_dim=vocab_size, output_dim=50, input_length=max_sequence_length))\n",
    "\n",
    "# Flatten the output of the Embedding layer\n",
    "model.add(layers.GlobalMaxPooling1D())\n",
    "\n",
    "# Hidden Layers\n",
    "model.add(layers.Dense(50, activation=\"relu\"))\n",
    "model.add(layers.Dropout(0.3))\n",
    "model.add(layers.Dense(50, activation=\"relu\"))\n",
    "model.add(layers.Dropout(0.2))\n",
    "model.add(layers.Dense(50, activation=\"relu\"))\n",
    "\n",
    "# Output Layer (3 units for 3 classes, with softmax activation)\n",
    "model.add(layers.Dense(3, activation=\"softmax\"))\n",
    "\n",
    "model.summary()\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer=\"adam\", loss=\"categorical_crossentropy\", metrics=[\"accuracy\"])\n",
    "\n",
    "# Train the model\n",
    "results = model.fit(\n",
    "    train_x, train_y,\n",
    "    epochs=5,\n",
    "    batch_size=32,\n",
    "    validation_data=(test_x, test_y)\n",
    ")\n",
    "\n",
    "train_accuracy = results.history['accuracy']\n",
    "val_accuracy = results.history['val_accuracy']\n",
    "train_loss = results.history['loss']\n",
    "val_loss = results.history['val_loss']\n",
    "\n",
    "\n",
    "\n",
    "#Testing on new data\n",
    "new_data = pd.DataFrame({\n",
    "    'tweet': [\n",
    "        \"JPMorgan price will fall. It's been a bad week for the company\", \n",
    "        \"Stock price soars on great day production increases at the company.\", \n",
    "        \"I am going to be looking at the stock price later today.\",\n",
    "        \"There has been a change in management at chipotle.\",\n",
    "        \"Profits soar for small startup with a big jump in sales.\"\n",
    "    ]\n",
    "})\n",
    "\n",
    "print(f\"Max sequence length from training data: {max_sequence_length}\")\n",
    "\n",
    "# Step 1: Preprocess the new data\n",
    "# Clean the new data\n",
    "pattern = r'[^\\w\\s]'  # Keep only words and spaces\n",
    "new_data['tweet'] = new_data['tweet'].apply(lambda x: re.sub(pattern, '', x))\n",
    "\n",
    "# Step 2: Tokenize the new data\n",
    "# Use the same tokenizer that was fitted on the training data\n",
    "sequences_new = tokenizer.texts_to_sequences(new_data['tweet'])\n",
    "\n",
    "# Step 3: Pad the sequences (using the defined max_sequence_length)\n",
    "padded_sequences_new = pad_sequences(sequences_new, maxlen=max_sequence_length, padding='post')\n",
    "\n",
    "# Step 4: Make predictions\n",
    "\n",
    "predictions = model.predict(padded_sequences_new)\n",
    "print(predictions)\n",
    "# Step 5: Convert predictions to class labels\n",
    "predicted_classes = np.argmax(predictions, axis=1)\n",
    "\n",
    "# Output the results\n",
    "for tweet, sentiment in zip(new_data['tweet'], predicted_classes):\n",
    "    sentiment_labels = ['Neutral', 'Positive', 'Negative']\n",
    "    print(f\"Tweet: '{tweet}' => Predicted Sentiment: {sentiment_labels[sentiment]}\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c6ff754-dcef-480d-9452-1a9435264e7f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "9bba79cc-3d96-446e-b12a-84b5a7194af6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "All PyTorch model weights were used when initializing TFBertForSequenceClassification.\n",
      "\n",
      "All the weights of TFBertForSequenceClassification were initialized from the PyTorch model.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertForSequenceClassification for predictions without further training.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tweet: JPMorgan price will fall. It's been a bad week for the company:              {'label': 'BEARISH', 'score': 0.9997867941856384}\n",
      "Tweet: Stock price soars on great day production increases at the company.:         {'label': 'BULLISH', 'score': 0.968792200088501}\n",
      "Tweet: I am going to be looking at the stock price later today.:                    {'label': 'NEUTRAL', 'score': 0.9666387438774109}\n",
      "Tweet: There has been a change in management at chipotle.:                          {'label': 'NEUTRAL', 'score': 0.9979617595672607}\n",
      "Tweet: Profits soar for small startup with a big jump in sales.:                    {'label': 'BULLISH', 'score': 0.9602739214897156}\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from keras.utils import to_categorical\n",
    "from keras import models\n",
    "from keras import layers\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "import re\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "new_data = pd.DataFrame({\n",
    "    'tweet': [\n",
    "        \"JPMorgan price will fall. It's been a bad week for the company\", \n",
    "        \"Stock price soars on great day production increases at the company.\", \n",
    "        \"I am going to be looking at the stock price later today.\",\n",
    "        \"There has been a change in management at chipotle.\",\n",
    "        \"Profits soar for small startup with a big jump in sales.\"\n",
    "    ]\n",
    "})\n",
    "\n",
    "\n",
    "#USing a pretrained function:\n",
    "from transformers import pipeline\n",
    "\n",
    "# Create a sentiment analysis pipeline\n",
    "pipe = pipeline(\n",
    "    \"sentiment-analysis\",\n",
    "    model=\"StephanAkkerman/FinTwitBERT-sentiment\",\n",
    ")\n",
    "\n",
    "list_results=pipe(new_data['tweet'].to_list())\n",
    "# Get the predicted sentiment\n",
    "for i, t in enumerate(new_data['tweet']):\n",
    "    print(f\"Tweet: {t}: {' '*(75-len(t))}{list_results[i]}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a01f3df9-9475-4fbd-92ea-6bf4f3a5235d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
